# Actions Workflow

This document describes the GitHub Actions workflow used for the purposes of CI (Continuous Integration) in Dynamatic. More details on parts of the CI can be found in the corresponding documents about [formatting checks](Formatting.md) and [integration testing](IntegrationTests.md).

## Basics

Workflows are defined using YAML files, placed in the `.github/workflows` directory. A workflow consists of one or more jobs, and each job consists of one or more steps. 

Workflows are triggered (i.e. run), when certain changes on the repository occur. For example, a workflow can run whenever a pull request into main is opened/marked ready for reviewm, or when a push is made into a specified branch, and so on. A comprehensive list of such trigger events is available [in the official documentation](https://docs.github.com/en/actions/reference/workflows-and-actions/events-that-trigger-workflows).

Each job is executed in a separate environment, i.e. jobs do not share anything by default. They can run either on GitHub's cloud-based runners, or on user-defined self-hosted runners.
- [GitHub's runners](https://docs.github.com/en/actions/concepts/runners/github-hosted-runners) are ephemeral virtual machines, i.e. they are created only for running that specific job and deleted after the job is finished. This means that they are unsuitable for jobs that require large dependencies, since they would need to be reinstalled every time.
- [Self-hosted runners](https://docs.github.com/en/actions/concepts/runners/self-hosted-runners) can be set up by the repository administrators on any personal/school PC. Since they are regular machines by themselves, one can manage them however he or she wants. This means that dependencies do not need to be installed on every run. The downside is obviously that one needs to own a machine for this and that the contention for it is likely going to be large.

Steps are executed in order on a single runner, as if you were to run the commands normally via the shell on your computer. If a step fails (returns a non-zero exit code), the job will stop and fail, unless otherwise specified.

The main Actions workflow for building and integration is described in [.github/workflows/ci.yml](/.github/workflows/ci.yml). It runs every time a pull request into main is opened, reopened, updated (i.e. new commits are pushed to it) or marked ready for review (i.e. converted from draft PR to "regular" PR). It consists of two jobs:
- `check-format`, which runs the [formatting checks](Formatting.md),
- `integration`, which runs the build process, [integration tests](IntegrationTests.md) and [MLIR unit tests](../IntroductoryMaterial/FileCheckTesting.md).

`check-format` runs on cloud runners, while `integration` runs on a self-hosted runner that is set up on a VM inside EPFL's network. This is because of the fact that `integration` requires a large number of dependencies which cannot be feasibly installed on every run on GitHub's runners, and also because it requires build artifact caching, which would be more difficult to set up with GitHub's ephemeral runners.

## Usage

You don't have to do anything manually to make the CI run. As soon as you open a PR (or do any of the other actions mentioned above), the workflow will run and jobs will get picked up by runners as soon as they can be.

A job fails if any one of its steps fail. You can see the outcome of each individual step (as well as the console output) by clicking on the job in the Actions UI:

![Actions job UI](Figures/actions.png)

Files generated by integration tests (Dynamatic shell output, MLIR intermediate compilation results, HDL files, simulation logs...) are archived by the CI and uploaded to GitHub as an artifact. You can navigate to the workflow summary, where you can find and download the artifact at the bottom:

![Actions artifact](Figures/artifact.png)

A new feature is the performance report. Integration test performance (in terms of cycles during the simulation) is collected and outputted in the form of a Markdown table. It is also located in the summary tab and looks like this:

![Performance report](Figures/perf.png)

The `cycles` column is the performance of the current branch's integration tests, while `old_cycles` is the performance that was last recorded in main. `result` is the outcome of the integration test. `comparison` is the difference `cycles - old_cycles`; positive is bad, negative is good. You can see the percent difference in the parentheses to the right. If any one test has a `comparison` percent value worse than 5% (this limit is defined by the option `--limit 5` that the script `generate_perf_report.py` is called with in `ci.yml`), the performance report will fail the CI and you will have a red exclamation mark at the top notifying you that something is not right. Of course, you can always ignore this if you deem the performance difference to be insignificant.

## FAQ

- What OS does the self-hosted runner use?

  - Ubuntu 24.04.2 LTS.

- How do I install dependencies on the self-hosted runner?
  
  - You can't; ask someone who has access.

- How do I make the CI do something I want it to do?

  - The CI workflow should be something "standard" and as such should not be changed without a good reason. If you do have one though, edit `.github/workflows/ci.yml` and explain your changes in your PR.

- How do I run the same thing that the CI does on my machine?

  - For integration tests, use `ninja -C build run-ci-integration-tests`. For formatting checks, see [Formatting](Formatting.md).

- The CI is failing the formatting check, but it passes it on my machine, what's wrong?

  - Make sure you have the same version of clang-format and autopep8 as the ones used in the CI. You can see which versions are used in the `ci.yml` files.

- Is there a possibility of leftover state from previous runs to affect my run on the self-hosted runner?

  - This used to be a problem, but in the current setup, the entire workspace folder is emptied before the `integration` job runs. The only thing that is shared is ccache's cache, which is located somewhere else on the runner. This is because without it, building Polygeist would take a very very long time. We trust that ccache does its job properly, but if you are certain that it is the culprit for some issue you are having, contact someone with runner access to clear the cache for you.